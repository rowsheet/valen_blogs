 As we develop and discuss our predictive models we like to point out that one of the key benefits is that they help underwriters avoid unfounded bias.  The models are developed based on data not on intuition or anecdotal experiences.  They represent ideally an optimal weighting of a set of input variables.In contrast the underwriter has built up a set of experiences that form a basis for an intuitive understanding of risk quality.  Some of these intuitions are well-founded; others are purely coincidental and not at all predictive.  The goal of the model in our experience is to give the underwriter a solid consistent base of understanding using a well-defined list of input variables.  This frees the underwriters to consider everything else outside of that small list of inputs.When presenting the model to a savvy group of underwriters we almost always get these interesting questions:Which variables does the model use?Why are those variables selected?Why is this other variable NOT in the model?The easy answer of course is that we listen to the data – we let statistics determine which variables are important and which are not and make our selections accordingly.  But what happens when the data says a variable is important and we disagree?  As model builders should we substitute our intuition for solid indications?  Or should we listen to the data and include variables to which we object?This is where model building starts to become an art as well as a science.  The answer to this question lies in understanding the intended application of the model and using that understanding to determine how strictly to follow the “pure” indications of the data.Here’s an example:  What are some of the variables that underwriters would universally say are predictive of future loss?  Probably every underwriter would agree that past losses are predictive of future losses.  Given a choice between two identical exposures one with no prior losses and one with several high-severity prior losses every underwriter will choose the first risk.When we model workers’ compensation risks we find that the data backs this up: prior loss history is important.  Technically speaking prior loss history is correlated with future loss ratio.  However there is a bit of a surprise here:  Prior loss severity is inversely correlated with future loss ratio.Let me restate that to be clear: Policies with large losses in prior years have LOWER future loss ratios.It gets worse: those lower future loss ratios are on a manual basis.  This means that all else being equal an underwriter should prefer (or give credit to) accounts with large losses.  What’s going on here?  Honestly we don’t know with certainty why this is true but we see it across multiple carriers so we believe that it’s generally true.  Some possible explanations:Employees at these accounts with large losses are more careful in the futureThey put safety equipment in placeThey receive loss control services and conduct safety trainingThey adjust their scope of operations to eliminate the source of prior lossIn the end it probably doesn’t matter why; the data is clear.  Past severity is predictive of the future loss ratio.  So what do we do with this?  Do we put the claim severity variable into the model?No.The only thing more important than having a model that works is having a model that the underwriter understands and trusts.  Without that trust the underwriter will simply ignore the model indications – or will only listen to the model when it agrees with their prior intuition.  This is equivalent to having no model at all.  If we include this variable in a final model we will be explicitly advising an underwriter to give preference to accounts with high-severity claims.  This will NOT engender trust.So we exclude past claim severity.  Yes we are deliberately removing a variable that shows predictive power because our end goal is not merely model power but impacting the bottom line through underwriter adoption.  We also do one more thing: we share all of these details with the underwriters so they’re aware not only of what’s IN the model but also what’s NOT IN the model.When considering the human element we’ve found that it’s just as important to talk about what’s present as what’s missing.  At the end of the day we want the underwriter to believe in the model that it’s doing the best job possible in considering what’s included.  At the same time we want the underwriter to focus on what’s not in the model because it’s there that the true expertise and value of underwriting is going to be found alongside predictive analytics. ABOUT THE AUTHOR:Bret Shroyer FCAS is an actuary and VP of Services for Valen Analytics. He serves as an advocate for Valen’s clients bridging the gap that sometimes grows between technical modeling and client service teams executives actuaries and underwriters. He helps the models drive success stories as they translate how data analytics helps people make better decisions and deliver tangible results.Bret joined Valen in 2014 after serving as SVP of Reinsurance at Willis Re for five years. From 2006 to 2008 Bret served as CFO of an environmental consulting and construction firm. Immediately prior to this Bret held numerous positions including Senior Actuary Underwriting Director and Predictive Modeling Manager during his ten-year tenure at Travelers.Bret earned a B.A. in Mathematics from the University of St. Thomas in St. Paul Minnesota and is a Fellow of the Casualty Actuarial Society. 
 News of carriers implementing predictive analytics has been relatively widespread the past several years. Even if you’ve been hiding under a rock it’s almost impossible to avoid hearing about how companies are turning around their results through better modeling or how new competitors like Lemonade and Zenefits are entering into insurance using the power of predictive analytics. It’s difficult to know when to begin especially when implementation seems like a daunting task.So when is the best time to adopt predictive analytics and implement a predictive model?The answer is simple: Now. Here’s why: Market share consolidation is happening.We’ve already seen the personal lines market become consolidated and now commercial lines are following suit. It’s no coincidence that the historically troubled line of workers’ compensation only became profitable for the first time since 2006 while significant shifts in market share appear among the top 10 insurers.Companies such as Travelers and Berkshire Hathaway are known to have a well-established holistic analytics strategy driving their businesses and have developed a more targeted and sophisticated customer acquisition strategy in order to locate the best risks and price them competitively.Adding to the consolidation issue is the continued growth of mergers and acquisitions on a global scale with the U.S. showing the most activity at a 61% year-over-year increase. Commercial insurers need to arm themselves with more data so they can compete with companies targeting niche markets and leveraging more granular segmentation of mainstream business.Having the tools needed to compete for this business is crucial to staying ahead of the pack during a tumultuous period of change in the industry. Outside competition is coming into insurance.Think you’re only competing against other insurance carriers? Think again. Investment in insurance technology more than tripled from 2014 to 2015 with a whopping $2.6 billion last year. Industry giants like Progressive and USAA are known for their innovation and analytics approaches but there are new players on the horizon with the likes of Lemonade Coverhound Policy Genius Everquote Compare.com and Zenefits just to name a few.All of this innovation comes at a time of immense disruption in response to the overall lack of customer satisfaction with the insurance industry. With an average industry-wide Net Promoter Score of 28 (out of 100) the fact is that we as an industry are not living up to customer expectations. This gap in satisfaction means that competitors who can offer a streamlined efficient experience can swoop in and show customers what they’ve been searching for. Your internal teams are fighting each other over price.Pricing has never been more important than it is right now. Thirty years ago carriers could earn a 16% ROE by simply breaking even on the underwriting side. Now with lower investment yields and lower leverage a 100% combined ratio delivers an anemic 3.5% ROE. To make profitability targets carriers have to be making a profit on the underwriting side. There is no other option.However in a recent survey we conducted with over 200 P/C insurance professionals we found that 77% believe that actuaries and underwriters are at odds over price. Setting prices has traditionally been seen as the domain of the actuary. But you also need smart risk selection which is the art of underwriting – knowing whether to accept the risk under consideration at prices being set by the market. If they’re working in opposite directions you’re going to go in circles. Your customers are demanding immediacy.With advances in technology people have become accustomed (and conditioned) to faster and faster responses which means underwriters are squeezed for time and have to generate quotes much quicker than they used to.Consider these examples:FirstComp conducted an in-depth study of response times to closing workers’ comp business. They found that when an insurer provides a quote to an agent within one hour the quote-to-bind ratio is 52%. However if you wait 24 hours it drops to 30%. Think back 5-10 years. Would anyone have talked about a 20% drop in the quote-to-bind ratio because an agent had to wait just one business day?Zurich understanding this trend in customer expectations cut the time required to give a commercial auto quote on a 100 vehicle fleet from 8 hours to just 15 minutes. That wasn’t their only win here – now their underwriters can focus on applying their expertise (actually underwriting) instead of spending time gathering and entering data.Both of these examples are from 2012 so imagine how much has changed in just 4 years. Your competition is already using predictive models to adversely select against you.Adverse selection blindsides many carriers with devastating consequences on their portfolios as early adopters have already started to become much more sophisticated in their pricing. Companies that underwrite and price based on traditional methods are competing against these early adopters. The companies with more sophisticated pricing are able to drive adverse selection by stealing the good risks and leaving the poor risks behind for their competition. So when is the best time to implement predictive analytics? There’s no time like the present. Carriers today are using advanced data and analytics to gain an advantage and the number of carriers armed with this kind of insight is increasing in a big way. This is a battlefield in which superior information defines the high ground. Those with access to this information can maintain their strategic position – defending their place on good risks and refusing to take on new bad risks. Those without it can’t.It’s time to take the first step. Here’s how. “You never change things by fighting the existing reality.To change something build a new model that makes the existing model obsolete.”–R. Buckminster FullerABOUT THE AUTHOR:Bret Shroyer FCAS is an actuary and VP of Services for Valen Analytics. He serves as an advocate for Valen’s clients bridging the gap that sometimes grows between technical modeling and client service teams executives actuaries and underwriters. He helps the models drive success stories as they translate how data analytics helps people make better decisions and deliver tangible results.Bret joined Valen in 2014 after serving as SVP of Reinsurance at Willis Re for five years. From 2006 to 2008 Bret served as CFO of an environmental consulting and construction firm. Immediately prior to this Bret held numerous positions including Senior Actuary Underwriting Director and Predictive Modeling Manager during his ten-year tenure at Travelers.Bret earned a B.A. in Mathematics from the University of St. Thomas in St. Paul Minnesota and is a Fellow of the Casualty Actuarial Society. 
 Whether or not predictive analytics is implemented in your insurance organization you are already using predictive models…the ones inside your underwriters’ heads. They may not realize it but combining the human experience of an underwriter’s skill and experience with a predictive model built on historical data leads to astounding results.As we noted in a survey of 111 property casualty executives underwriters are resistant to using predictive analytics. Because of this Valen conducted a study of a regional P/C insurer to show how powerful it can be when there isn’t a resistance to using analytics in underwriting. The chart below is used to measure lift which in terms of predictive modeling is how we decide the effectiveness of the model. In short – the higher the lift the better the model.We started by asking: What predictive power did their underwriters have? (You can do this at your company too.) We see the underwriters’ predictions by looking at what tier they placed a risk in or how much debit or credit they selected (represented by the green line above). The underwriting organization used in this study did in fact separate risks from best quality to worst quality. The best risks had a loss ratio about 20% better than average while the worst were 30% to 50% worse than average – not bad at all.Next we looked at a predictive model (represented by the red line above). This improved the overall differential between “best and worst” – the best risks are 50% better than average and the worst are about 75% worse than average.Finally we took the average of the underwriter’s prediction and the model’s prediction which is represented by the blue line on the graph. It has by far the most lift from 75% better than average all the way up to 120% worse than average.This study is indicative of results we see consistently – the combination of human expertise and analytics is the winning combination. Despite underwriters’ fear of predictive analytics people still remain an insurer’s most valuable asset. Remember that analytics are a tool and as with most tools the people controlling them decide its efficacy.In a time when innovation is disrupting every market especially insurance it’s best to be well informed and understand where your new and real competition exists. In order to alleviate the pressure from this tech driven competition it takes more than simply becoming data-driven yourself. To elicit the best results from your predictive modeling carriers must focus on creating a predictive analytics strategy the whole organization can get behind.Want to know what else to focus on in 2016?Download the 2016 P/C Outlook Report now.ABOUT THE AUTHOR:Bret Shroyer FCAS is an actuary and Solutions Architect for Valen Analytics. He serves as an advocate for Valen’s clients bridging the gap that sometimes grows between technical modeling and client service teams executives actuaries and underwriters. He helps the models drive success stories as they translate how data analytics helps people make better decisions and deliver tangible results. Bret joined Valen in 2014 after serving as SVP of Reinsurance at Willis Re for five years. From 2006 to 2008 Bret served as CFO of an environmental consulting and construction firm. Immediately prior to this Bret held numerous positions including Senior Actuary Underwriting Director and Predictive Modeling Manager during his ten-year tenure at Travelers.Bret earned a B.A. in Mathematics from the University of St. Thomas in St. Paul Minnesota and is a Fellow of the Casualty Actuarial Society. 
 The property/casualty insurance industry has long been driven by market cycles. As prices rise in hard markets carriers increase rates wherever the market will let them. Then when prices fall in soft markets carriers make the opposite choice and see how low they will let prices go before throwing in the towel and letting a lower-priced competitor take the good business.A common misconception about market cycles is they are caused by reactions to changes in loss ratio- when losses start trending up the market reacts with higher prices. So when the market overreacts and increases prices too much it results in very low loss ratios increased competition and prices decreasing into a softening market. But a recent Willis Re study debunks this notion.Every year Raj Bohra from Willis Re looks at market cycles by line of business. Bohra compiled a graph of past workers’ compensation market cycles as shown in the image below. In this aggregate view of the work comp industry results the blue line represents accident year loss ratio and the red line represents price. We see volatility in both but is price reacting to loss ratio or are movements in loss ratio a result of changes in price?Take a look at the green line representing the historic loss rate per dollar of payroll. Contrary to the other two lines this one has been virtually flat since 1995 meaning that on an aggregate basis there has been no fundamental change in loss rate for the past 20 years. All of this points to one driver: price movement. The industry has caused the cycles itself.Of course there will always be shock industry events or shifting trends that will drive price changes so market cycles will never truly disappear but they need to be relied on less when making pricing decisions. We see now that market cycles are led by pricing inefficiency and more carriers are becoming increasingly sophisticated with pricing through the use of predictive analytics rather than reacting to broader market trends. The result? The sophisticated pricing is putting an end to extreme market cycles.When carriers know what they insure they can make rational pricing decisions at the account level regardless of how the larger market is pricing. In a hard market they can accumulate the best new business by correctly offering them quotes below the market price. So when they’re in a soft market they can shed the worst renewal business to their naïve competitors.Consider this:We recently conducted a study of our commercial lines partners’ portfolios. Using their own data and predictive models we helped them answer two key questions:What’s our potential benefit in using predictive analytics when our competition doesn’t?What’s our potential vulnerability if the competition uses predictive analytics and we don’t?The results were surprising. On average we found:Companies had 9 loss ratio points of potential advantage from using advanced analyticsCompanies not yet using analytics faced an average vulnerability of nearly 20 pointsAs we move into a soft market how will you respond with this new insight? Will you let the market drive or will you decide where you play in the cycle?Get the 2016 Outlook ReportFor a Better View of the Road AheadABOUT THE AUTHOR:Bret Shroyer FCAS is an actuary and Solutions Architect for Valen Analytics. He serves as an advocate for Valen’s clients bridging the gap that sometimes grows between technical modeling and client service teams executives actuaries and underwriters. He helps the models drive success stories as they translate how data analytics helps people make better decisions and deliver tangible results. Bret joined Valen in 2014 after serving as SVP of Reinsurance at Willis Re for five years. From 2006 to 2008 Bret served as CFO of an environmental consulting and construction firm. Immediately prior to this Bret held numerous positions including Senior Actuary Underwriting Director and Predictive Modeling Manager during his ten-year tenure at Travelers.Bret earned a B.A. in Mathematics from the University of St. Thomas in St. Paul Minnesota and is a Fellow of the Casualty Actuarial Society. 
 There’s little doubt about the proven value in using predictive analytics for risk selection and pricing in P/C insurance. In fact at this year’s Valen Analytics Summit insurance executives reported that many of them were already using predictive analytics for underwriting and the 56% that were not planned to start within a year. However many insurers haven’t spent enough energy planning exactly how to implement analytics to get the results they want. While insurers would like predictive analytics to be a one-size-fits-all improvement to their business it is a custom built tool where the degree of success is determined by the mutual understanding of the benefits and rules surrounding the predictive model and how it works within the specific organization.There are various steps that need attention along the implementation journey like gaining organizational buy-in and assessing your organization’s capabilities and resources. However the first step when considering predictive analytics is to secure senior level commitment.Information is a business enabler and any data analytics project must produce meaningful insights that will solve particular problems and achieve specific objectives. Once you define the problem to solve make sure that all the relevant stakeholders understand the business goals from the beginning and that you have secured executive commitment or sponsorship.Next the organization must agree up front upon a set of metrics which will measure the success of the implementation. Questions you will need to answer include:How will we prove that a predictive model will produce results? What is our proof of concept?What are the agreed upon metrics to measure success? These include loss ratio price competitiveness premium growth?What management reporting will you put in place? If it can be measured it can be managed.How do we know if a predictive model is giving us new insights vs. telling us what we already know?What is our risk appetite for this initiative? What are the assumptions and sensitivities in our model and how will those impact projected results?What is the plan to integrate the model within our existing workflow?Valen’s recent survey showed that loss ratio was the top issue for underwriting analytics. Whether it’s loss ratio pricing competitiveness premium growth or something else create a baseline so you can show before and after results with your analytics project. Remember to start small and build on early wins; don’t boil the ocean right out of the gate. Pick a portion of your policies or a test group of underwriters and run a limited pilot project. That’s the best way to get something started promptly prove you have the right process in place and make sure to scale as you see success.Finally consider the risk appetite for any particular initiative. What are the assumptions and sensitivities in your predictive model and how will those affect projected results? Don’t forget to think through how to integrate the model within your existing workflow.Get the Step-by-Step Guide forSecuring Executive Sponsorship 
 After studying the portfolios of commercial lines insurers we discovered that 10% of their books contributed over 50% of expected profit across a range of companies and lines of business including work comp BOP and commercial auto. This is a prime example of the threat – as well as the advantage – of data analytics. The carrier who accurately identifies the best 10% of the market will be able to compete on and win this business while the carrier who can’t will lose out.This is why predictive analytics is rapidly emerging as a must-have tool in the insurance carrier’s arsenal. Predictive analytics can be deployed from the bottom up to look at everything from individual account pricing new business underwriting triage audit and inspection optimization claims management and incident-level fraud detection. Alternatively predictive analytics can also be deployed from the top down. Imagine if you could accurately predict your accident-year loss ratio every year by the end of June? Could you improve performance if you were armed with this kind of powerful insight?With embedded profit a forward-looking view of profitability at the policy level this type of proactive approach to your book of business is made possible. Embedded profit = collected premium – (predicted loss + predicted expense) and it offers a number of important advantages:It is credible at the policy level.It provides real-time feedback on profitability before policy inception (not 12-24 months later).It doesn’t depend on loss development or IBNR assumptions.It provides a new statistic which is very useful for other applications.These applications of predictive analytics while novel represent just an introduction to the possibilities. The shift to predictive analytics opens the door to a completely new class of metrics – real-time and forward-looking – that can fundamentally change the ways in which an insurance portfolio is managed. Embedded profit is just the tip of the iceberg.That being said implementing predictive analytics in an underwriting organization is a significant undertaking which many companies mistakenly equate success to selecting the right predictive model. However the model is only one part of a larger process. Whether you are developing a model in-house or using an external technology provider there are components common to every predictive analytics project that are necessary to ensure success. This punch list includes 3 step-by-step guides that will best prepare your organization to get started. 
 When carriers who are having profitability problems for a segment of business attempt to correct the issue they all too often go about it the wrong way. These are the steps we see on a very regular basis:Put in place a 10% rate increase. (After which loss ratio actually deteriorates.)Find a poor-performing class or region and exit that business. (Volume drops but results don’t materially improve.)Resort to predictive analytics to find an answer.In reality these efforts should have started with the third step above and moved forward from there. Beginning with predictive analytics the carrier can identify the policies or groups of policies that are performing better or worse than expected. This almost never results in the conclusion that barbers (sorry I had to pick on someone) should be non-renewed or that the carrier simply needs to stop writing in Allenstown. Instead it points to the policies in each of those groups that are predicted to be trouble as well as identifies the policies in those groups that are predicted to be profitable.Farm Bureau Financial Services which writes workers compensation coverage is one of the exceptions to this rule. In 2012 they were in some trouble so they quickly implemented predictive analytics into their BOP commercial lines to improve underwriting profitability and turn the ship around. As a result they not only reduced their loss ratio by 66% from 2012 to 2014 they also grew their premium by 22.4% in 2013 and 10.1% in 2014.As Farm Bureau learned there’s something surprising that happens when you start looking at predictive statistics on an in-force book. The model doesn’t have pre-conceived notions about which regions or classes are in or out of favor. As a result within the worst-performing class the model might find that 25% of the policies are significantly better than average and should be retained. Conversely within the best-performing class the model might find that 25% are significantly worse than average and should be cut.This is how carriers experience significant wins when implementing predictive analytics for the first time. They discover many opportunities to make significant improvements in risk selection and pricing on the in-force book and for the first time they can actually see and measure the impact of adverse selection on their book in recent years. There are so many opportunities to gather this low hanging fruit that it’s hard to choose a bad place to start once you have a reliable model in place.As a result the preferred order of operations becomes:Implement predictive analytics to look at prospective profitability at the policy level.Use that information to drive risk selection and pricing at the policy level. This will result in non-renewing or losing some segments and growing other segments. These segments emerge organically from the bottom up rather than as a general strategy from the top down.After re-underwriting and re-pricing the book adjust base rate level for portfolio profitability target.The final result is a portfolio comprising risks that you want to write at adequate prices. It may even include some Allenstown barbers.ABOUT THE AUTHOR:Bret Shroyer FCAS is an actuary and Solutions Architect for Valen Analytics. He serves as an advocate for Valen’s clients bridging the gap that sometimes grows between technical modeling and client service teams executives actuaries and underwriters. He helps the models drive success stories as they translate how data analytics helps people make better decisions and deliver tangible results. Bret joined Valen in 2014 after serving as SVP of Reinsurance at Willis Re for five years. From 2006 to 2008 Bret served as CFO of an environmental consulting and construction firm. Immediately prior to this Bret held numerous positions including Senior Actuary Underwriting Director and Predictive Modeling Manager during his ten-year tenure at Travelers.Bret earned a B.A. in Mathematics from the University of St. Thomas in St. Paul Minnesota and is a Fellow of the Casualty Actuarial Society.  Here’s how Farm Bureau won the2015 Celent Model Insurer Award  
 There is a common misconception in insurance that competitive advantage is won by simply picking the right predictive model. However the model itself is just a small part of a much larger process that touches nearly every part of the insurance organization. Embracing predictive analytics is like recruiting a star quarterback; alone he’s not enough to guarantee a win. He requires both a solid team and a good playbook to achieve his full potential.Get the full organizational readiness checklist hereThat being said here’s a list of 4 key steps that are imperative to ensuring your predictive analytics implementation is a success throughout your organization:Alignment of PeopleThe most important first step in predictive modeling is making sure all relevant stakeholders understand the business goals and organizational commitment. The number one cause of failure in predictive modeling initiatives isn’t a technical or data problem but instead a lack of clarity on the business objective combined with a defect in the implementation plan (or lack thereof).Senior level commitment and organizational buy-in are both key here. Decide on metrics that management will use to measure the impact of the model what problems you are trying to solve and how you will define success. You must also ask yourself: If we lead will they follow? Data analytics can only be developed and deployed in the right environment. You have to retool your people so that underwriters don’t feel that data analytics are a threat to their expertise or actuaries to their tried-and-true pricing models.Resource AssessmentA predictive analytics engagement is done in-house or by a consultant or built and hosted by a modeling firm. Regardless of whether the data analytics project will be internally or externally developed your assessment should be equally as rigorous. It’s critical to consider:Data resources: Do you have enough in-house or will you need external sources to help you fill in the gaps?Modeling best practices: Whether internal or external do you have a solid approach to data custody data partitioning model validation and choosing the right type of model for your specific application?IT resources: Ensure that scope is accurately defined and know when you will be able to implement the model. If you are swamped by an IT backlog of 18-24 months you will lose competitive ground.Reporting: If it can be measured it can be managed. Reporting should include success metrics easily available to all stakeholders along with real-time insights so that your underwriters can make changes to improve risk selection and pricing decisions.Predictive Model ImplementationUnless you’re lucky enough to work with an entire group of like-minded individuals this step must be taken with all players involved including underwriting actuarial training and executive roles. Once you’ve identified the business case and produced the model and implementation plan make sure all expected results are matched up with the planned deliverables. Once everything is up and running it is imperative to monitor the adoption in real-time to ensure that the results are matching the initial model goals put in place.Underwriter TrainingA very important but often overlooked step is making sure that underwriters understand why the model is being implemented what the desired outcomes are and what their role is in implementing it. If the information is presented correctly underwriters understand that predictive modeling is a tool that can improve their pricing and risk selection as opposed to undermining the underwriters. But there are still some who rely solely on their own experience and knowledge who may feel threatened by a data-driven underwriting process. In fact nearly half of the attending carriers at the 2015 Valen Summit cited lack of underwriting adoption as one of the primary risks in a predictive analytics initiative.Boiling this down what’s critical is that you align a data analytics initiative to a strategic business priority. Once you do that it will be far easier to garner the time and attention required across the organization. Remember incorporating predictive analytics isn’t just about technology. Success is heavily dependent on people and process.Want to know what other questions you should be asking?Get your checklist now 
 “If you aren’t using analytics to increase your efficiency cut waste and make better business decisions you are in danger of being left behind.” – Bernard Marr This could not be truer especially in the insurance world where adverse selection is blindsiding many carriers with devastating effects on their portfolios. Those that have predictive analytics on the other hand are able to locate and price the best risks at more desirable rates while avoiding poor performing risks at insufficient prices. The carriers not leveraging data analytics are then left with poorly performing accounts while their best business is taken from them often without even realizing it until it’s too late.If you don’t know where you stand you’re already a victim. So how do you know if you are a victim of adverse selection and what can you do about it to protect your book? There are three indicators that should raise red flags that you are in the danger zone.The first is ‘climbing loss ratios and loss costs’. If a carrier frequently blames market conditions and the irrational pricing of risk by competitors it could be an early warning sign that the competition has better data to accurately price a specific risk.The second is if a carrier’s rates go up while their volume declines. When raising rates puts policy retention at risk but maintaining the same rates cuts into profitability their business is stuck in a losing situation if an actuary finds that manual rates aren’t able to cover the expected future cost.The third is when a carrier has inadequate reserves. When actuaries show signs of mild reserve inadequacy the claims department often argues that while reserving practices are the same loss frequency and severity have increased. This forces insurers to downsize and focus on a niche specialization in order to survive while also diminishing possible future growth – a very precarious situation for any insurer to be in. The fundamental problem with this is that the insurer cannot identify and price risk with the accuracy that their competitors can.If any of these sound familiar you are likely being adversely selected against by carriers utilizing data analytics. Your book may be quietly suffering but you can turn it around.In a recent Towers Watson survey more than half the participants using predictive modeling said it helped with policy retention and 87 percent reported seeing profitability with the use of their models. These carriers realize early significant wins when focused on the following three areas at the outset of a predictive modeling implementation:Improved results: Find the specific problem accounts and increase price adequacy or drop the risk.Profitable growth: Be more competitive on the better risks and win more of the good business being submitted.Underwriting consistency: Understand what’s in the book and build underwriting processes to standardize risk selection and pricing strategies.Carriers today are using advanced data and analytics to gain an advantage. Superior information defines the high ground. Those with access to this information can maintain their strategic position defending their place on good risks and refusing to take on new bad risks. Unfortunately it’s the carriers without access to this information that will prove to be the casualties in this fight.Which side will you be on?photo credit: MFer04 via photopin (license) 
 Predictive analytics is a very powerful tool; however a major point of failure is reliance on it as a magic bullet.  The largest risk isn’t technical or theoretical (i.e. something wrong with the model) as many would believe but rather a risk in execution.  The predictive model is only one part of the analytics solution.  It’s just a tool and it needs to be managed well to be effective.This shifts the burden of risk from the statisticians and model builders to the managers and executives.  The insurer may have an organizational readiness problem or a management and measurement problem which means the fatal flaw that’s going to derail a predictive analytics project isn’t in the model but in the implementation plan.Perhaps the most common manifestation of this is when the implementation plan around a predictive model is forced upon a group instead of using it as a tool to enhance the effectiveness of each person’s role:Underwriters are told that they must non-renew accounts above a certain scoreActuaries are told that the model is now going to determine the rate planManagers are told that the model will define the growth strategyIn each of these cases the plan is often meant to replace human expertise with model output which almost never ends well.One company that has had such an exemplary implementation that they won a 2015 Celent Model Insurer Award in Data Mastery and Analytics is Farm Bureau Financial Services.  They were struggling with workers’ compensation underwriting performance was deteriorating rapidly and the competition was outperforming them.  So immediate action was required to bring their workers’ compensation line of business back to profitability.True pros Farm Bureau implemented predictive analytics into their underwriting process with the best execution plan we’ve seen.  They created a process that was easy for everyone using their model to understand the output and they implemented it in a consistent way across the board.  They empowered their underwriters with new tools to make better decisions and as a result they reduced their loss ratio by 66% in just two years.Predictive models deliver a steady stream of data that can (and should) help to steer the portfolio.  By utilizing these prospective statistics underwriters can see in real time the impact of their risk selection and pricing decisions on future profitability – but only if they have the business intelligence tools in place to give them this feedback.  If an insurer implements predictive analytics without also executing a system of measurement and monitoring they’ll only see half of the story.Let us not forget the immortal words of Spider-Man: “With great power comes great responsibility.”  Insurers investing the time and expense of implementing predictive analytics should feel the responsibility to put in place the support systems needed to extract maximum value from the models.Get the Celent Case Study to LearnHow You Can Replicate Farm Bureau’s Successphoto credit: The seeing eye via photopin (license) 
 As P&C carriers enter into predictive analytics implementation there are three broad categories of development and implementation:The third party develops and maintains the model.The third party develops the model and the carrier implements and maintains it.The carrier develops houses and maintains the model.For each of these options there are a number of pros and cons; this article will guide the reader through a practical evaluation of each potential course.Click Here to Find out How Predictive Analytics Is Being Implemented Today
 It’s not hard to find discussions of the benefits and new opportunities presented by predictive analytics in commercial lines underwriting.  I’m certainly doing my best to add to this body of work.  For all of this new opportunity however it’s important to remember that predictive analytics cannot be successful without a solid implementation plan.  I would even go so far as to say the number one cause of failure that insurers see in predictive analytics projects is not a technical or theoretical problem with the model but a more fundamental problem with a poorly designed or executed implementation plan.In this article I’ll explore just a few of the more popular modes of failure being experienced by insurers today.Failure: Let’s Just Score the Large RisksSimilar failure: “Let’s just score the small risks.”The value of a predictive model decreases rapidly when less than 100% of the risks are scored.  Think about it: the value proposition of a predictive model is that it can help us take advantage of patterns and correlations that we are unlikely to find on our own.  If we pre-suppose that we only need help on large risks we’re making two big mistakes.We’re losing the value of insights we might otherwise receive from the model on small risks.We’re supplying less data to train the model.In predictive analytics data is everything.  More data means more predictive power.  If you’ve got a model you should be scoring the stuff you write the stuff you don’t want to write the stuff you wrote five years ago new business renewals and declinations.  Score everything.When you score everything you start to see patterns emerge.  This isn’t the top-level pattern the one that tells you what’s preferred and what’s not.  It’s the pattern that tells you what your pricing and risk selection really look like:Are we retaining the best business?Are we declining on the worst business?Which agents are sending us good opportunities?Are we pricing well to risk?Are we experiencing adverse selection?All of these questions can have different answers by region or class of business or size of risk – so it’s important to score everything and let the models tell you where your strengths and opportunities are.  Don’t assume you know the answer – you may not even know if you’re asking the right question.Failure: Let’s Just Get Rid Of XYZHere’s a typical order of operations for a carrier that’s having profitability problems for a segment of business:Put in place a 10% rate increase. Loss ratio actually deteriorates.Find a poor-performing class or region and exit that business. Volume drops but results don’t materially improve.Unsatisfied with above results resort to predictive analytics to find an answer.In reality these efforts were taken in exactly the wrong order.  Starting with predictive analytics the carrier could identify the policies or groups of policies that are performing better or worse than expected.  This almost never results in the conclusion that all hairdressers (sorry I had to pick on someone) should be non-renewed or that the carrier needs to simply stop writing in River City.  Instead it points to the particular policies in each of those groups that are predicted to be trouble and also identifies the policies in those groups that are predicted to be profitable.There’s something surprising that happens when you start looking at predictive statistics on an in-force book.  The model doesn’t have pre-conceived notions about which regions or classes are in or out of favor.  As a result within the worst-performing class the model might find that 25% of the policies are significantly better than average and should be retained.  Conversely within the best-performing class the model might find that 25% are significantly worse than average and should be cut.As a result the preferred order of operations becomes:Implement predictive analytics to look at prospective profitability at the policy level.Use that information to drive risk selection and pricing at the policy level. This will result in non-renewing or losing some segments and growing other segments.  These segments emerge organically from the bottom up rather than as a general strategy from the top down.After re-underwriting and re-pricing the book adjust base rate level for portfolio profitability target.The final result is a portfolio comprising risks that you want to write at adequate prices.  It may even include some River City hairdressers.Failure: Believing That Predictive Analytics Will Save UsThe final big point of failure is reliance on predictive analytics as a magic bullet.  It’s not.  It’s merely a tool – albeit a very powerful one.  Let us not forget the immortal words of Spider-Man: “With great power comes great responsibility.”  Insurers investing the time and expense of implementing predictive analytics should feel the responsibility to put in place the support systems needed to extract maximum value from the models.Predictive models deliver a steady stream of data that can (and should) help to steer the portfolio.  By utilizing these prospective statistics underwriters can see in real time the impact of their risk selection and pricing decisions on future profitability – but only if they have the BI tools in place to give them this feedback.  If an insurer implements predictive analytics without also executing a system of measurement and monitoring they’re only seeing half of the story.We already have a well-established feedback loop in insurance: analyze loss ratio assess performance and adjust.  The big problem here is that the loss ratio statistic is often lagged six months to a year (or more) after the underwriting actions were taken.  Predictive statistics on the other hand can give us information on our risk selection and pricing performance up to a month in advance of the policy inception date.  This allows carriers to take corrective action if they aren’t getting immediate value from a predictive model implementation.Avoid FailureSo you’re on the road to implementing predictive analytics and want to avoid some of the most common points of failure?  Here’s the big three from this article:Cast the predictive analytics net as far and wide as possible given the data available.Let analytics drive tactical priorities not the other way around.Don’t just implement analytics. Also put in place a system for real-time monitoring and adjustment. Want to learn more about how predictive analytics is being implemented today in insurance?Click here now.  
 When predictive analytics is implemented by an insurance company executives typically have several pressing questions:How will we know it’s working?How do we measure return on investment?How can we extract more value from the model?Are the underwriters using the models as we expected?Are we experiencing adverse selection?To get answers to these questions execs need access to a new set of metrics based not on past performance but on predicted future performance.  These questions can only be answered using predictive statistics.  I will highlight two statistics in particular that when used together and in conjunction with traditional metrics shed new light on insurer performance. Forget the what. We need to know why.For decades insurers have measured performance using a standard set of statistics:Loss ratioCombined ratioRate changeFrequency and severity of lossHit ratioPremium volumeThese statistics have several things in common.  First they are all measures of past performance.  Second (and most importantly) they answer what happened but they are all very poor at telling us why it happened.Let’s take loss ratio for example.  (I love to pick on loss ratio.)  We may determine we wrote to a 70% loss ratio this year and a 75% loss ratio last year.  Why?  There are three distinct options:It’s what the market gave us our agents sent us better new business or benefit schedules changed resulting in lower average severity.It’s what we did in risk selection or pricing.  We were better at finding good risks or losing bad risks or we just increased rate leaving risk quality static.Dumb luck.  Last year we had a couple of big claims and this year we haven’t…yet.This is a classic problem for the C-Suite management.  How can you build strategy if you don’t know what’s really driving current results – and therefore don’t know what’s going to have an impact in the future? Use Predictive Statistics to Find Out “Why”Luckily there is a solution to this problem.  There are statistics that give a laser focus to the drivers and answer the difficult “Why” questions.These two statistics borne out of predictive analytics point to the answer:Predicted Risk QualityPredicted Loss Ratio RelativityPredicted Risk Quality expressed as a relativity to starting value shows changes in risk quality over time.  This is a primary output of a predictive model and shows us whether the inforce book is increasing or decreasing in quality over time.  This is an important statistic and should marry with the carrier’s strategy.If the carrier is using predictive analytics to target the worst account for rate increases or non-renewal you should see risk quality increasing over time as we expect the mix to be shifting toward better risks. Predicted Risk QualityIf the company is seeing average risk quality worsen over time (as shown in this graph) we might suspect that adverse selection is at work.  This graph shows that the policies incepting this month have 10% higher loss potential than the baseline portfolio at 1/1/2013. Predicted Loss Ratio RelativityThe second statistic Loss Ratio Relativity is again a prospective measure giving us the expected loss ratio on these newly incepting policies vs. the baseline loss ratio.  This measure incorporates (and in fact requires) the calculation of Risk Score – but includes price information as well.  In other words predicted Loss Ratio Relativity tells us how the underwriters have reacted to the Risk Quality prediction. Putting Them Together (It’s OK to Cross the Streams)The final graph illustrates combining these two statistics to identify a successful predictive analytics implementation.  Loss Ratio Relativity is decreasing so results are improving.  However Average Risk Quality is relatively unchanged which indicates that the carrier is probably pricing better to risk particularly on high-risk accounts.A more accurate pricing and risk selection model allows this carrier to expand its underwriting box with assurance that they can get price right.  They’re losing some high-risk accounts to the market but are keeping the portfolio in balance.  Loss ratio is improving but not because the risks are getting better; rather because the risks are more accurately priced.These two trends – Risk Quality and Loss Ratio Relativity – combine to reveal the effectiveness of a carrier’s predictive modeling efforts.  By evaluating predictive statistics we can finally start answering the hardest question in predictive analytics: “Are my models actually delivering value to justify the investment?”photo credit: .sarahwynne. via photopin cc 
 Bernard Marr is one of the big voices to pay attention to on the subject of Big Data.  His recent piece “Big Data: The Predictions for 2015” is bold and thought provoking.  That is it got me to thinking.  As a P&C actuary I tend to look at everything through my insurance-colored glasses.  So of course I immediately started thinking about the impact on insurance if Mr. Marr’s predictions come to pass this year.As I share my thoughts below be aware that the section headers are taken from his article; the rest of the content are my thoughts and interpretations of the impact to the insurance industry.The value of the big data economy will reach $125 billionThat’s a really big number Mr. Marr.  I think I know how to answer my son the next time he comes to me looking for advice on a college major.  (More on that below as well.)But what does this huge number mean for insurance?  There’s a potential time bomb here for Commercial Lines because this $125 billion means we’re going to see new commerce (and new risks) that are not currently reflected in loss history – and therefore not reflected in rates.Maybe premiums will go up as exposures increase with the new commerce – but that begs a new question: What’s the right exposure base for aggregating and analyzing big data?  Is it revenues?  Data observation count?  Megaflops?  We don’t know the answer to this yet.  Unfortunately it’s not until we start seeing losses that we’ll know for sure.The Internet of Things will go mainstreamWe already have some limited integration of “The Internet of Things” into our insurance world.  Witness UBI (Usage-Based Insurance) which can tie auto insurance premiums to not only miles driven but also driving quality.Google’s Nest thermostat keeps track of when you’re home and away whether you’re heating or cooling and communicates this information back to a data store.  Could that data be used in more accurate Homeowners pricing?  If so it would be like UBI for the house.The Internet of Things can extend to healthcare and medical insurance as well.  We already have health plans offering a discount for attending the gym 12 times per month.  We all have “a friend” that sometimes “checks in” at the gym to meet the quota and get the discount.  With the proliferation of worn biometric devices (FitBit Nike Fuel and so on) it would be trivial for the carrier to offer a UBI discount based on the quantity and quality of the workout.  Of course they’d need to get the policyholder’s permission to use that data but if the discount is big enough we’ll buy it.Machines will get better at making decisionsAs I talk with carriers about predictive analytics this concept is one of the most disruptive to underwriters and actuaries.  There is a fundamental distrust that the model is going to replace them.Machines are getting better at making decisions but within most of insurance and certainly within commercial lines the machines should be seen as an enabling technology that helps the underwriter to make better decisions or the actuary to make more accurate rates.  Expert systems can do well on risks that fit neatly into a standard underwriting box but anything outside of that box is going to need some human intervention.Textual analysis will become more widely usedA recurring theme I hear in talking to carriers is a desire to do claims analysis fraud detection or claims triage using textual analysis present in the claims adjusters’ files.  There are early adopters in the industry doing this and there have emerged several consultants and vendors offering bespoke solutions.  I think that 2015 could be the year that we see some standardized “off the shelf” solutions emerge that offer claims predictive analytics using textual analysis.Data visualization tools will dominate the marketThis is spot-on in insurance too.  Data visualization and exploration tools are emerging quickly in the insurance space.  The lines between “reporting tool” and “data analysis tool” are blurring.  Companies are realizing that they can combine Key Performance Indicators and metrics from multiple data streams into single dashboard views.  This leads to insights that were never before possible using single dimension standard reporting.There is so much data present in so many dimensions that it no longer makes sense to look at a fixed set of static exhibits when managing insurance operations.  Good performance metrics don’t necessarily lead to answers but instead to better questions – and answering these new questions demands a dynamic data visualization environment.Matt Mosher SVP Rating Services at A.M. Best will be talking to this point in March at the Valen Analytics Summit and exploring how companies embracing analytics are finding ways to leverage their data-driven approach across the entire enterprise.  This ultimately leads to significant benefits for these firms both in portfolio profitability and in overall financial strength.There will be a big scare over privacyHere we are back in the realm of new risks again.  P&C underwriters have long been aware of “cyber” risks and control these through specialized forms and policy exclusions.With big data however comes new levels of risk.  What happens for example when the insurance company knows something about the policyholder that the policyholder hasn’t revealed?  (As a thought experiment imagine what Google knows of your political affiliations or marital status even though you’ve probably never formally given Google this information per se.)  If the insurance company uses that information in underwriting or pricing does this raise privacy issues?Companies and organizations will struggle to find data talentIf this is a huge issue for big data in general then it’s a really really big deal for insurance.I can understand that college freshmen aren’t necessarily dreaming of a career as a “Data Analyst” when they graduate.  So now put “Insurance Data Analyst” up as a career choice and we’re even lower on the list.  If we’re going to attract the right data talent in the coming decade the insurance industry has to do something to make this stuff look sexy starting right now.Big data will provide the key to the mysteries of the universeNow it seems Mr. Marr has the upper hand.  For the life of me I can’t figure out how to spin prognostication about the Large Hadron Collider into an insurance angle.  Well played.Those of us in the insurance industry have long joked that this industry is one of the last to adopt new methods and technology.  I feel we’ve continued the trend with big data and predictive analytics – at least we certainly weren’t the first to the party.  However there was a tremendous amount of movement in 2013 and again in 2014.  Insurance is ready for big data.  And just in time because I agree with Mr. Marr – 2015 is going to be a big year. Industry execs JOIN US at the Valen Analytics Summitalong with Patrick Byrne CEO of Overstockand Matthew Mosher FCAS MAAA CERA Senior VP – Rating Services at A.M. Bestamong other featured speakers and industry CEOs to explore theincreasingly rapid adoption of data and analytics within insurance companiescombined with the potential impact from technology companies entering insurance. 
 (With apologies to Charles Dickens)It was Christmas Eve and most of the other department heads had sent their staff home early.  Ebenezer however was still hard at work as his department kept a tight consistent schedule.  “We’ve always done it this way. There’s no reason to change now!” he proclaimed to his cubicle-imprisoned staff who were at that moment finishing up the Missouri rate filing.Ebenezer was the Chief Pricing Actuary for Time Tested Insurance a post he had held for forty years.  He was a legend in the industry not only for his stalwart consistency but also for some of his pioneering work in rate plan development.  He was “The E” from the “E-J Method” of multivariate rating.  Bob Cratchit a first year intern was excited at the chance to work with “The E” when he joined Time Tested but this evening he just wanted to finish up his work and join his family.Finally at 5:00 on the nose Ebenezer released his staff.  The rate filing was ready to go and once again his department was right on schedule.  Ebenezer was pleased.  He didn’t hear (or rather he tried not to notice) the grumbles from his staff as they hastily made their way out of the building.Marley VisitsLater that night as Ebenezer laid in bed he was visited by the spirit of his old co-worker Jacob Marley – “The J.”  Jacob appeared to Ebenezer heavily weighted in chains and he spoke to his old friend.  “E you have to change!  Look at me weighted down by my past.  Univariate analyses.  Flat files.  COBOL.  I’m going to carry these chains forever but it’s not too late for you!  E my friend tonight you’ll be visited by three spirits.  Listen to what they have to say!”The Ghost of Insurance PastSoon after Ebenezer was visited by the Ghost of Insurance Past.  The spirit beckoned Ebenezer on a journey into the past and showed him visions from his younger years.  Ebenezer marveled at the simplicity of it all.  Independent agents were in charge.  Data needs were minimal and computing was hard – but companies that could build more sophisticated rate plans were big winners.  Reputation was king; companies competed (and won) on their good name and financial strength.It was a world very familiar to Ebenezer.  It was the insurance industry he grew up in the industry that he first fell in love with.  But Ebenezer now saw it with apprehension.  He recognized the many things that had changed things that could no longer be.And then just like that the spirit departed and Ebenezer was alone.The Ghost of Insurance PresentShortly after Ebenezer was visited by the Ghost of Insurance Present.  The spirit called to Ebenezer and showed him how things were in present day including many things that Ebenezer had not previously noticed.  He saw the increasingly savvy direct purchasers of insurance not relying on agents to guide them.  He saw carriers using new sorts of data in very novel ways.  He saw adverse selection emerge as a primary vulnerability – and for some carriers a key competitive weapon.The spirit showed Ebenezer how the rules were changing with data and analytics; that even small companies could now perform very sophisticated pricing analysis and that the key differentiator now was not reputation but data.  The carriers with the most data and the ability to analyze it were in the driver’s seat.Finally the spirit showed Ebenezer something he had never noticed before: the interlopers.  There at the fringes testing the waters looking to break into the insurance market were scores of non-insurance firms.  Social media networks online retailers and brick and mortar stores all had plans to compete with traditional insurance companies.  They were empowered by the depth and breadth of their customer data but the insurance companies didn’t seem to notice this new threat.Ebenezer was amazed.And then just like that the spirit departed and Ebenezer was alone.The Ghost of Insurance FutureImmediately the third spirit appeared: the Ghost of Insurance Future.  He showed Ebenezer an amazing future the likes of which he could hardly believe.  There were no recognizable “Insurance Companies” any longer – just consumer service companies that packaged insurance products with financial products consulting services data and physical products.The spirit showed Ebenezer how data and analytics had evolved to their natural conclusion: the insurance product was no different in the end from a communications product a financial product or the sale of a physical product.  Firms that could successfully combine all of these data streams into one view were ultimately the most successful at leveraging profit across the entire spectrum.In this world data and analytics weren’t merely important things.  They were the only things that mattered.  Executing in a Data-Driven Decision making (D3) culture was the new key to organizational success.The independent agents were gone replaced by financial consultants and customer service reps who helped buyers manage their insurance needs within the scope of their larger packages.The carriers that didn’t embrace these changes were also gone.  Without the data to compete they lost their best business year after year to the new market and slowly dwindled away.  There were plenty of familiar names among the successful carriers but without fail they had partnered with (or were owned by) other consumer services and products companies.Ebenezer saw that his beloved Time Tested Insurance stood shuttered and abandoned and he gasped in shock.And then just like that the spirit departed and Ebenezer was again alone.Of Course There’s A Happy EndingWhen his staff returned to work after Christmas Ebenezer was a changed man.  “Data and analytics aren’t merely important” he told his staff.  “They’re the only things that matter!”He ordered that the Missouri filing be put on hold (it would be his first missed filing date in three decades) while a team studied the underlying cause of their loss ratio shift in the state.He petitioned management to begin an Organizational Readiness Assessment for a shift to D3.He commissioned a task force to begin rebuilding a data warehouse – not using extracts and encodings of customer data but straight-through storage of every data point on every transaction.  “We’ll keep it all and figure out how to analyze it later” he told his staff.And finally he did something for the intern Bob Cratchit.  Since Bob was both new and enthusiastic he hired Bob into a permanent position and set Bob up as the point person to evaluate predictive analytics tools and options.  “I want the whole department to embrace advanced analytics.  Bob you go out and survey the market the tools and the classes we should be taking to get up to speed. “And Bob did.  And the department did.  And with Ebenezer’s help Time Tested grew and thrived and blessed its employees with a lavish 401(k) match.  Every one.photo credit: Mraz Center for the Performing Arts via photopin cc 
 Dear Loss RatioThere’s so much I want to say to you but I’m not sure where I should begin.  Should I start by telling you how much you’ve taught me?  Or that the years we’ve spent together in the insurance industry have been the happiest of my life?  I could say these things – and they would be true – but I have to admit (and I’m sorry if this is painful for you) that I’ve found a new ratio.When I first met Predicted Loss Ratio (I call her Polly) she was just coming out of a new predictive model.  Polly wasn’t overtly flashy so I almost didn’t notice her at first.  However something about her reminded me a lot of you – at first anyway – and we started spending some time together.  That’s when I noticed so many ways that you and Polly are actually different.Polly is always thinking of the future and helps me see exactly where I’m going.  You on the other hand are always stuck in the past always pointing out where I’ve just been.  With you I always know that you’ll help point out my mistakes but not until after the fact while Polly helps me see more clearly what I’m doing today and how to avoid some of those mistakes.Polly is so…predictable.  When she talks to me she’s transparent.  I can understand not only what she’s saying to me but why.  In a way I guess you could say that Polly is almost scripted or formulaic.  When I talk to you Loss Ratio I sometimes feel lost.  You are so opaque.  You tell me you have bad news but then you make me play twenty questions before you tell me why.Loss Ratio compared to Polly you’re just so…random.  I used to find our random walks together interesting but now that I’ve met Polly I have to say that I’m enjoying the stability.  It’s just that you are always changing your mind.  I’ve come to accept the fact that you’ll give me one answer today but then you’ll tell me something different next year or even next month.When I go out on insurance business meetings Polly always comes with me and speaks with power when I’m talking with small accounts or individual agents.  You never have any credibility when you talk about any of these little things; you only have credibility when you could talk about the big picture.In a way I think maybe your number one shortcoming is that you are always too immature.  For years I’ve felt like I’ve been waiting for you to develop.  Polly on the other hand was fully mature on the day we first met.Loss Ratio you’ve taught me so much over the years so I think we should still be friends.  Really I don’t think I can ever truly let go because you’re still such an authority on so many things.  We should still get together a couple times a year though just to touch base and see how we’re doing but from now on when I’m making plans or thinking of the future I’m going to stick with Polly. SincerelyBret photo credit: Free Grunge Textures – www.freestock.ca via photopin cc 
 It’s that time of the year again when we pause to give thanks for our many blessings.  With apologies to the Mayflower voyagers at the risk of sounding irreverent I’d like to give thanks in this column for advances in predictive analytics. Thank you fast computersOne of the foundational methods in predictive analytics the GLM (Generalized Linear Model) was first introduced in 1972.  At that time the only way to actually perform a GLM analysis was via mainframe.  The state of the art in 1972 was the IBM Model S/370 165 which operated at 17 MHz and cost nearly $3 million dollars if you purchased two 64MB hard drives (each as big as a washing machine).  Needless to say not a lot of folks were playing around with GLM in those days – most of the work was being done “on paper” or “in theory” long before the instructions were finally sent off to the mainframe for calculation.We now have available on our desktops (or laptops) processors with a thousand times more processing power and 10000 times more storage for less than 1/1000 the price.  That’s a million times more performance for the same price! Thank you big dataBig data depends first on big storage.  Even in the 1990s storage was an issue.  At my first job in the insurance industry I learned that we stored our policyholder and claim data on magnetic tapes each holding 20GB of storage and that those tapes were housed in a silo 50 miles away where a robot would change the tape in the drive when I requested particular data.  We had thousands of tapes in the silos – something like 40 terabytes of total storage.Nevertheless we agonized over every byte we stored.  We would compress data convert continuous data to categorical then assign those categories codes to assist in more efficient storage.  I can’t imagine how much of our policyholder data we weren’t capturing along the way in our effort to decrease the storage burden.With the advent of cheap storage (you can store 40 terabytes on a dozen USB drives now) the need to re-factor data and build structured data warehouses has been lifted.  The new mantra is to “just save it all” because storage prices will keep coming down and our ability to process all of that unstructured data will someday catch up with the mountain of data we’ve accumulated.There’s signal in that data so don’t throw any away.  What could be easier? Thank you free educationIt’s amazing the places you can go to get a free education in predictive analytics these days. The following are great resources to get started:Coursera.org offers full university courses on dozens of data analytics topics.Kaggle.com conducts predictive modeling contests – and has a vibrant community of enthusiasts willing to review your code or give you feedback on how to improve your methods.Codecademy.com will teach you a new programming language absolutely free. Thank you free researchWith predictive modeling entering the mainstream more and more academic research is being performed on analytical methods and more companies are also publishing results of their research.  So much of the heavy lifting has already been done.  Sometimes the solution to your problem is just as easy as finding someone out there who has already faced the same problem solved it and published the results.A few of the outstanding resources here include:The Elements of Statistical Learning by Hastie Tibshirani and FriedmanData Analysis and Graphics Using R by John MaindonaldUser forums discussing projects and competitions on Kaggle.com Thank you free toolsMy very first foray into the world of predictive modeling was in the late 90’s.  At that time the equipment I was using included a server array an Oracle Database and SAS software.  The total cost was somewhere around $100000 and this was just a small development project.  For carriers with models in production at the time the hardware and software costs easily went into the millions.Over time many of the most expensive parts of predictive analytics have seen the release of free and/or open source alternatives:Linux high-performance server O/SMySQL and PostgreSQL database systemsR and KNIME data analyticsHadoop and MongoDB for Big Data storage access and distributed processing And these are just a few examples – the list and the examples go on and on.Predictive analytics has come a long ways in the last 40 years.  It started as a very theoretical effort led by a team of data scientists and executed on multi-million dollar hardware running multi-million dollar software.  Today predictive analytics can be tackled as a hobby by a high school student taking free courses to learn how to use free software on an aging notebook computer.  Perhaps because of this predictive analytics has fully entered the mainstream.  It’s not seen as an indecipherable black box but now as one of the fundamental building blocks of business data analysis.We’ve come a long way and it feels like we’re just getting started.  For that too I’m thankful. If you’d like to explore more about underwriting analytics big data and the technology revolution in insurance be sure to register for the Valen Summit in March with keynote speaker Patrick Byrne (CEO Overstock) and Featured Guest Speakers including Matt Mosher from A.M. Best.photo credit: carlaarena via photopin cc 
 News and discussion of predictive analytics and big data is everywhere these days.  Even if you’ve been hiding under a rock the last 25 years it’s almost impossible to avoid hearing about how companies are turning around their results through better modeling or how new companies are entering into insurance using the power of predictive modeling.So now you’re ready to embrace what the 21st century has to offer and explore predictive analytics.  Though predictive modeling and analytics have finally arrived as a mainstream tool in property/casualty insurance this doesn’t mean that misconceptions aren’t still commonplace.  Here below are the top 6 myths dispelled.Myth: Predictive modeling is mostly a technical challengeFact: The predictive model is only one part of the analytics solution.  It’s just a tool and it needs to be managed well to be effective.The number one point of failure in predictive analytics isn’t technical or theoretical (i.e. something wrong with the model) but rather a failure in execution.  This shifts the burden of risk from the statisticians and model builders to the managers and executives.  The carrier may have an organizational readiness problem or a management and measurement problem.  The fatal flaw that’s going to derail a predictive analytics project isn’t in the model but in the implementation plan.Perhaps the most common manifestation of this is when the implementation plan around a predictive model is forced upon a group:Underwriters are told that they must non-renew accounts above a certain scoreActuaries are told that the model are now going to determine the rate planManagers are told that the models will define the growth strategyIn each of these cases the plan is to replace human expertise with model output.  This almost never ends well.  Instead the model should be used as a tool to enhance the effectiveness of the underwriter actuary or manager.Myth: The most important thing is to use the right kind of predictive modelFact: The choice of model algorithm and the calibration of that model to the available data is almost never the most important thing although it is still significant.  Instead it’s merely having a credible body of data upon which to build a model which is the biggest challenge.  In their article “The Unreasonable Effectiveness of Data” Google research directors Halevy Norvig and Pereira wrote:“Invariably simple models and a lot of data trump more elaborate models based on less data.”No amount of clever model selection and calibration can overcome the fundamental problem of not having enough data.  If you don’t have enough data you still have some options: you could supplement in-house data with third party non-insurance data append insurance industry aggregates and averages or you may be able to make use of a multi-carrier data consortium as we are doing here at Valen.Essentially if you get everything wrong it doesn’t matter if you have the right kind of predictive model.Myth: It really doesn’t matter which model I use as long as it’s predictiveFact: Assuming you have enough data to build a credible model there is still a lot of importance in choosing the right model – though maybe not for the reason you’d at first think.The right model might not be the one which delivers the most predictive power; it also has to be the model which has a high probability of success in application.  For example if the model is one which underwriters will use to help them make better business decisions then choose a model which has transparency and is intuitive not a model which relies on complex machine learning techniques.Similar to the previous myth if you use the wrong predictive model it doesn’t matter that you get everything right.Myth: Predictive modeling only works well for personal linesFact: Personal lines were the first areas of success for predictive modeling owing to their large homogeneous populations.  This isn’t to say that commercial lines are immune to the power of predictive modeling however.  There are successful models in production today producing risk scores for Workers Compensation E&S Liability and even Directors & Officers risks.  One of the keys to deploying predictive models to lines with thin policy data is to supplement that data either with industry-wide statistics or with third-party (not necessarily insurance) data.Myth: Better predictive modeling will give me accurate prices at the policy levelFact: Until someone invents a time machine the premiums we charge at inception will always be wrong.  For policies that end up being loss-free we will charge too much.  For the policies that end up having losses we will charge too little.  This isn’t a bad thing however.  In fact this cross subsidization is the fundamental purpose of insurance and is necessary.Instead of being 100% accurate at the policy level the objective we should aim for in predictive analytics is to segment the entire portfolio of risks into smaller subdivisions each of which is accurately priced.  See the difference?  Now the low risk policies can cross-subsidize one another (and enjoy a lower rate) and the high risk policies will also cross-subsidize one another (but at a high rate).  In this way the final premiums charged will be more fair for every policyholder.Myth: Good predictive models will give me the right answersFact: Good models will answer very specific questions but unless you’re asking the right questions your model isn’t necessarily going to give you useful answers.  Take time during the due diligence phase to figure out what the key questions are.  Then when you start selecting or building models you’ll be more likely to select a model with answers to the most important questions.For example there are (at least) two very different approaches to loss modeling:Pure premium (loss) models can tell you which risks have the highest potential for loss. They don’t necessarily tell you why this is true or whether or not the risk is profitable.Loss Ratio models can tell you which risks are the most profitable where your rate plan may be out of alignment with risk or where the potential for loss is highest. However they may not necessarily be able to differentiate between these scenarios.Make sure that the model is in perfect alignment with the most important questions to receive the greatest benefit from predictive analytics. photo credit: YaelBeeri via photopin cc 
